# spiderWeb


爬虫站点

	请求不成功 使用旧缓存
	先返回 再更新缓存或统一管理缓存定时更新(总之不能占用请求-返回的时间)
	可以监控各节点的网络使用情况 如果空闲 可以作为爬虫节点或多分担采集任务

